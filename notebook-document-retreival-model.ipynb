{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n\nfrom datasets import load_dataset\n\n# Load the dataset\n\ndataset = load_dataset('not-lain/sroie')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the entire structure of the dataset to inspect available fields\nprint(dataset)\n\n# Print the first example in the training set to see what keys it contains\nprint(dataset['train'][0].keys())\n\n# Print the first example to see the actual data\nprint(dataset['train'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom transformers import LayoutLMv3Processor\nimport numpy as np\nimport torch\nfrom datasets import Dataset\n\n# Clear CUDA cache before training\ntorch.cuda.empty_cache()\n\n# Initialize the processor with apply_ocr=True\nprocessor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=True)\n\ndef preprocess_data(examples):\n    def convert_image(img):\n        if isinstance(img, np.ndarray):\n            if img.ndim == 2:\n                img = np.stack([img] * 3, axis=-1)\n            return Image.fromarray(img).convert(\"RGB\")\n        elif isinstance(img, Image.Image):\n            return img.convert(\"RGB\")\n        else:\n            raise TypeError(\"Unsupported image type\")\n\n    images = [convert_image(img) for img in examples['images']]\n    encoded_inputs = processor(images, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n\n    labels = examples.get('labels', [0] * len(images))  # Default to zeros if labels are missing\n    encoded_inputs['labels'] = torch.tensor(labels)\n\n    return encoded_inputs\n\ndef process_dataset_in_batches(dataset, batch_size=8):\n    \"\"\"Process the dataset in small batches to save memory.\"\"\"\n    all_batches = []\n    num_batches = len(dataset) // batch_size + (1 if len(dataset) % batch_size != 0 else 0)\n\n    for i in range(num_batches):\n        batch = dataset[i * batch_size:(i + 1) * batch_size]\n        processed_batch = preprocess_data(batch)\n        all_batches.append(processed_batch)\n\n    # Combine all the processed batches into a single dictionary\n    combined_batch = {key: torch.cat([b[key] for b in all_batches], dim=0) for key in all_batches[0]}\n    \n    return Dataset.from_dict({k: v.tolist() for k, v in combined_batch.items()})\n\n# Process the training and test datasets in smaller batches\ntrain_dataset = process_dataset_in_batches(dataset['train'], batch_size=8)\ntest_dataset = process_dataset_in_batches(dataset['test'], batch_size=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import LayoutLMv3ForTokenClassification, AdamW\nfrom tqdm import tqdm\n\n# Create dataloaders from the processed datasets\ndef create_dataloader(dataset, batch_size=8):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ntrain_dataloader = create_dataloader(train_dataset, batch_size=8)\ntest_dataloader = create_dataloader(test_dataset, batch_size=8)\n\n# Initialize the model\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=len(train_dataset.features[\"labels\"].feature.names))\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\ndef train(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        optimizer.zero_grad()\n        inputs = {key: value.to(device) for key, value in batch.items()}\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# Evaluation loop\ndef evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs = {key: value.to(device) for key, value in batch.items()}\n            outputs = model(**inputs)\n            loss = outputs.loss\n            total_loss += loss.item()\n    return total_loss / len(dataloader)\n\n# Train and evaluate the model\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    train_loss = train(model, train_dataloader, optimizer, device)\n    val_loss = evaluate(model, test_dataloader, device)\n    print(f\"Training Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}\")\n\n# Save the model\nmodel.save_pretrained(\"path_to_save_model\")\nprocessor.save_pretrained(\"path_to_save_processor\")\n\n# Load the model (if needed later)\n# model = LayoutLMv3ForTokenClassification.from_pretrained(\"path_to_save_model\")\n# processor = LayoutLMv3Processor.from_pretrained(\"path_to_save_processor\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nimport numpy as np\n\ndef evaluate_with_metrics(model, dataloader, device):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            inputs = {key: value.to(device) for key, value in batch.items()}\n            outputs = model(**inputs)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n            labels = inputs[\"labels\"].cpu().numpy()\n\n            # Mask padding labels\n            attention_mask = inputs[\"attention_mask\"].cpu().numpy()\n            predictions = np.where(attention_mask == 1, predictions, -100)\n            labels = np.where(attention_mask == 1, labels, -100)\n\n            all_preds.extend(predictions.flatten())\n            all_labels.extend(labels.flatten())\n    \n    # Remove -100s from both lists\n    valid_preds = [p for p, l in zip(all_preds, all_labels) if l != -100]\n    valid_labels = [l for l in all_labels if l != -100]\n\n    print(classification_report(valid_labels, valid_preds))\n    \n# Evaluate on the test set\nevaluate_with_metrics(model, test_dataloader, device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import LayoutLMv3ForTokenClassification, LayoutLMv3Processor\nimport torch\nfrom PIL import Image\n\n# Load the model and processor\nmodel = LayoutLMv3ForTokenClassification.from_pretrained(\"path_to_save_model\")\nprocessor = LayoutLMv3Processor.from_pretrained(\"path_to_save_processor\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef predict_on_new_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    encoded_inputs = processor([image], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    inputs = {key: value.to(device) for key, value in encoded_inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n    \n    predicted_labels = predictions.cpu().numpy()[0]\n    tokens = processor.tokenizer.convert_ids_to_tokens(encoded_inputs['input_ids'][0].cpu().numpy())\n\n    return list(zip(tokens, predicted_labels))\n\n# Example inference\npredictions = predict_on_new_image(\"path_to_new_image.png\")\nprint(predictions)","metadata":{},"execution_count":null,"outputs":[]}]}